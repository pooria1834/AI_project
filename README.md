# پروژه درس هوش مصنوعی
**اعضای تیم(افراد به‌ ترتیب حروف الفبا)**</br>
پوریا رحمانی     ۴۰۲۱۱۱۴۱۸</br>
نیما ملایی       ۴۰۲۱۰۶۵۵۳</br>
محمد‌رضا منعمیان  ۴۰۲۱۰۶۶۰۴</br>
## شرح مختصر در‌مورد فاز اول پروژه
<p dir="rtl">
در این فاز، وظیفه کلی ما تشخیص جاده از روی تصویر هوایی نقشه‌ی یک مکان بود و داده‌های آموزش، validation و تست ما همگی از دیتاست معروف جاده‌های ماساچوست بودند.</br>
این فاز، یک پروژه classification بود(بطوریکه باید برای هر تصویر، یک تصویر خروجی می‌ساختیم که هر پیکسل آن یا صفر بود(به معنای این که این پیکسل در تصویر اصلی بخشی از پس‌زمینه است) یا یک(به معنای اینکه این پیکسل در تصویر اصلی بخشی از یک جاده است)، بنابراین، به‌نوعی ما باید هر پیکسل از تصویر را classify می‌کردیم.</br>
در این فاز، از داده‌های برچسب‌گذاری شده برای آموزش و validation استفاده می‌شد، به‌گونه‌ای که هر داده یک جفت تصویر و mask بود که هر درایه از mask یا صفر بود یا 255 که 255 به معنای این بود که پیسکل متناظر در تصویر متناظر، بخشی از یک جاده است.</br>
تولید تصویر خروجی، یکبار با استفاده از مدل Unet یکبار با استفاده از مدل attention_Unet و یکبار هم با استفاده از مدل residual attention Unet انجام شد.</br>
</p>

### ساختار کلی Unet 
<p dir="rtl">
</br>
ساختار کلی Unet به این صورت است که چند لایه encode داریم یک bottleneck و سپس متناظر هر لایه encode، یک لایه decode!
</br>
در هر لایه encode ، روی تصویر، دوبار convolution(و یک Relu بعد هر convolution) سه در سه اعمال می‌کنیم، در convolution اول ، تعداد کانال‌ها دوبرابر می‌شود و در convolution دوم، تعداد کانال‌ها ثابت می‌ماند، سپس با استفاده از Max-pooling ابعاد هر کانال از تصویر را نصف می‌کنیم. می‌دانیم استفاده از convolution به کشف الگو‌های محلی(الگو بین چند پیکسل مجاور هم) فارغ از مکان آن‌ها در تصویر کمک می‌کند و کوچک کردن تصویر هم باعث می‌شود الگو‌های کلی‌تری با دامنه دید بیشتری کشف کنیم. پس با این فرایند، هم می‌توانیم الگو‌های محلی و جزئئ را کف کنیم و هم الگو‌های کلی تر و جامع تر را که مربوط به بخش بزرگی از تصویر هستند.
</br>
در لایه‌های decode هم تلاش می کنیم با توجه به اندوخته خود ، تصویر خروجی را باز‌سازی کنیم. از آنجا که ابعاد تصویر خروجی باید با ابعاد هر کانال تصویر ورودی یکسان باشد، باید تصویر را بزرگ کنیم. به این‌صورت که ابتدا در bottleneck دوبار روی تصویر convolution اعمال می‌کنیم بطوریکه تعداد کانال‌های تصویر در نهایت دو‌برابر می‌شود. حال در هر لایه decode، ابتدا یک عملیات upsampling برای بزرگ‌کردن تصویر انجام می‌دهیم(بطوریکه تعداد کانال‌ها نصف، اما ابعاد هر کانال دوبرابر شود) سپس تصویر خروجی لایه encode متناظر را به آن concat می‌کنیم و دوبار convolution روی آن اعمال می‌کنیم، بطوریکه در convolution اول ، تعداد کانال‌ها نصف شود و در convolution دوم، تعداد کانال‌ها ثابت بماند(توجه کنید برای اینکه بتوانیم عملیات concat کردن را انجام دهیم، باید ابعاد هر کانال از تصویر خروجی لایه encode با آنچه پس از upsampling داریم یکسان باشد، برای این منظور، یا می‌توانیم با استفاده از padding در convolution ها، مطمئن شویم convolution ، ابعاد هر کانال از تصویر را تغییر نمی‌دهد، که در پروژه ما این‌ گزینه را انتخاب کردیم، یا اینکه کانال‌های خروجی لایه encode را crop کنیم تا به ابعاد مناسب دربیایند)
</br>
فایده این concat کردن این است که بتوانیم اطلاعات جزئی را که با کوچک‌کردن تصویر از دست داده‌ایم، دوباره بدست آوریم و از آن‌ها برای تولید تصویر خروجی دقیق‌تر استفاده کنیم.
</br>
در نهایت ، یک convolution با تعداد کانال خروجی یک می‌زنیم و سپس تصویر را از تابع sigmoid رد می‌کنیم تا یک تصویر تک کاناله با درایه‌های بین صفر و یک داشته باشیم. درنهایت برای تولید تصویر باینری مد‌نظر می‌توانیم هر پیکسل از این‌تصویر که بیشتر از 0.5 بود را به 1 و هرکدام کمتر از 0.5 بود را به صفر تغییر دهیم.
</p>

### ساختار کلی Attention Unet
<p dir="rtl">
تفاوت Attention Unet با Unet عادی در بخشی است که تصویر خروجی لایه encode را به تصویر لایه decode پس از upsampling ، متصل(concat) می‌کنیم. در Unet عادی ، تصویر خروجی لایه encode به‌صورت خام ، concat می‌شود اما در Attention Unet مکانیزم به‌گونه‌ای است که به‌مرور بخش‌های مهم تصویر بیشتر مورد‌ توجه قرار بگیرند.</br>
در این مدل، تصویری که به تصویر لایه decode ، متصل می‌شود، خروجی attention gate است. attention gate ، درواقع تابعی‌است که ورودی های آن، تصویر خروجی لایه encode متناظر و خروجی لایه decode پایین‌تر است و خروجی آن حاصلضرب درایه به درایه خروجی خام لایه encode و ماتریسی است که با ترکیب‌کردن دو ورودی بدست می‌آوریم. ترکیبی که ذکر شد به این صورت بدست می‌آید: </br>
ابتدا، تعداد کانال‌ها و ابعاد کانال‌های این دو ورودی را با دو convolution با تعداد کانال خروجی یکسان، padding برابر صفر و stride های یک و دو، یکسان می‌کنیم تا دو ماتریس سه‌بعدی قابل جمع پیدا کنیم. سپس این دو ماتریس را با هم جمع‌می‌کنیم و حاصل جمع را از تابع Relu رد می‌کنیم و پس از یک convolution و sigmoid ، ابعاد هر کانال حاصل را با مکانیزمی که در فیلم‌ها توضیح‌داده خواهد شد به ابعاد مناسب برای concat شدن به تصویر لایه decode برمی‌گردانیم.</br>
منطق این ترکیب کردن این است که خروجی لایه‌ decode پایین‌تر داده‌های global تر و معنایی‌تری در خود دارد و به‌طور شهودی مثلا می‌داند جاده‌ها به‌طور کلی ظاهر می‌شوند و ...، در‌حالیکه خروجی لایه encode متناظر، اطلاعات جزئی‌تر و local‌ تری در‌خود دارد، همه این اطلاعات local ممکن است در تشخیص مکان حاده‌ها مهم نباشند، ترکیب این اطلاعات، با اطلاعات global تر باعث می‌شود به‌مرور روی اطلاعات با‌معنی‌تر و مهم‌تر تمرکز کنیم.
</p>


### ساختار کلی Residual attention Unet

<p dir="rtl">
 تفاوت Residual attention Unet و Attention Unet معمولی، در این است که attention Unet در‌هر لایه از بلوک‌های convolution عادی(شبیه آنچه در Unet معمولی استفاده می‌شود) استفاده می‌کند ولی Residual attention Unet از بلوک‌های Residual استفاده می‌کند که سساختار کلی آن را در پایین می‌آوریم: </br>
 در تصویری که در بالای cell مربوط به Residual attention در پروژه آمد، سه بار convolution روی ورودی می‌زنیم اما در template، دو convolution را باید کامل می‌کردیم، ما هم با همان دو بار convolution (بعلاوه Relu) ، بلوک را پیاده‌سازی کردیم.</br>
 تفاوت بلوک residual با بلوک convolution عادی، این است که خروجی این بلوک، به‌جای اینکه صرفا حاصل دو‌بار convolution روی ورودی بلوک باشد، حاصل جمع خود ورودی با حاصل این‌دوبار convolution است. البته اگر ورودی بلوک و حاصل دوبار convolution از نظر ابعاد یکسان نبودند(که در مدل ما هم تعداد کانال‌های این دو تفاوت دارد) با یک convolution (بدون Relu تا اعداد منفی در تصویر ورودی از بین نروند)، تصویر ورودی را به ابعاد مناسب در‌می‌آوریم.
 </br>
 یکی از فواید این جمع کردن این است که حتی اگر convolution باعث تخریب ویژگی‌ها شود، جمع کردن با ورودی‌ها، نمی‌گذارد تاثیر ویژگی تخریب شده کاملا از بین برود.
</p>

### فرایند آموزش

<p dir = "rtl">
 برای loss ، از bce + dice + iou استفاده کردیم که توضیح هر‌کدام در پاسخ به یکی‌از سوالات مربوط به همین بخش، در ویدیوی مربوطه می‌آید.
</br>
با‌توجه به امتیازات خواسته شده، در فرایند آموزش خود، از augmentation های بسیار ساده‌ای برای تصاویر استفاده کردیم و از هیچ augmentation ای برای mask ها استفاده نکردیم، از scheduler هم استفاده نکردیم و optimizer خود را هم، همان Adam نگه داشتیم و به کران‌های امتیازی خواسته‌شده رسیدیم!
</p>

## شرح مختصر درمورد فاز دوم پروژه
